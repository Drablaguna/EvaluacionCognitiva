{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud Notebook\n",
    "#### En este notebook podremos analizar con más facilidad la similitud entre las respuestas calificadas con mayor calificación con las de menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo librerias\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "spa_lex = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el CSV original como un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Drablaguna/Desktop/UNAM/EvaluacionCognitiva/cuenta_de_palabras/WORDCOUNT_SECUNDARIA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para eliminar caracteres de puntuacion\n",
    "def evalChar(x):\n",
    "    if x in [\",\",\".\",\":\",\";\",\"`\",\"'\",'\"',\"(\",\")\",\"-\",\"_\",\"~\",\"/\",\"?\",\"¿\",\"=\"]:\n",
    "        x = \"\"\n",
    "    return x\n",
    "def cleanString(x):\n",
    "    s = map(evalChar, x)\n",
    "    s = \"\".join(list(s))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para inicializar el diccionario de resultados, para tener resultados diferente por prueba\n",
    "def start_dict():\n",
    "    dic = {\n",
    "    'token_base': [],      # tokens originales\n",
    "    'token_toCompare': [], # token con el que se comparo\n",
    "    'similarity': [],      # valor de la similitud de ambos\n",
    "    'total_sim_mean': 0    # promedio similitud por frase\n",
    "    }\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para analizar\n",
    "def analize(base, comparer):\n",
    "    dic = start_dict()\n",
    "    for token_base in base:\n",
    "        for token_toCompare in comparer:\n",
    "            t1 = token_base.text\n",
    "            t2 = token_toCompare.text\n",
    "            simil = token_base.similarity(token_toCompare)\n",
    "            dic['token_base'].append(t1)\n",
    "            dic['token_toCompare'].append(t2)\n",
    "            dic['similarity'].append(simil)\n",
    "    # genero el promedio de similitud que fue extraido de la sim de cada token con token\n",
    "    dic['total_sim_mean'] = np.mean(dic['similarity'])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para analizar rapido y mostrar similitud\n",
    "def quickize(master_row, row):\n",
    "    master_row = spa_lex(master_row)\n",
    "    row = spa_lex(row)\n",
    "    dic = analize(master_row, row)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debajo pondremos la respuesta a ser comparada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_row = spa_lex('por que hace mucho ruido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debajo pondremos la respuesta que compararemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = spa_lex('que es fastidioso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutamos el analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analize(master_row, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado (SECU_TODO_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 a 1: 0.7051820238431294\n"
     ]
    }
   ],
   "source": [
    "# promedio similitud entre respuestas\n",
    "print('3 a 1: ' + str(results['total_sim_mean']))\n",
    "#print('\\n')\n",
    "#print(list(zip(results['token_base'], results['token_toCompare'], results['similarity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado (WORDCOUNT_SECUNDARIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 a 1: 0.50617975\n"
     ]
    }
   ],
   "source": [
    "# aqui se han retirado las STOPWORDS\n",
    "# de acuerdo al siguiente criterio de quickPlotter.py usando Spacy\n",
    "# if token.is_stop or token.is_punct or token.is_quote or len(token) == 1\n",
    "\n",
    "r = quickize('ruido', 'fastidioso')\n",
    "print('3 a 1: ' + str(r['total_sim_mean']))\n",
    "#print('\\n')\n",
    "#print(list(zip(results['token_base'], results['token_toCompare'], results['similarity'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 a 1: 0.32015392\n"
     ]
    }
   ],
   "source": [
    "r = quickize('persona gritona','hermano etsa gordo pasa gritando')\n",
    "print('3 a 1: ' + str(r['total_sim_mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 a 3: 0.7727020084857941\n",
      "3 a 3: 0.44147807\n",
      "3 a 3: 0.5552541\n",
      "\n",
      "\n",
      "3 a 3: 1.0\n",
      "3 a 3: 0.4979604\n",
      "3 a 3: 0.66183156\n"
     ]
    }
   ],
   "source": [
    "r = quickize('hermano grita','grita')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))\n",
    "\n",
    "r = quickize('hermano grita','ruido')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))\n",
    "                   \n",
    "r = quickize('hermano grita','gritos')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "r = quickize('grita','grita')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))\n",
    "\n",
    "r = quickize('grita','ruido')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))\n",
    "                   \n",
    "r = quickize('grita','gritos')\n",
    "print('3 a 3: ' + str(r['total_sim_mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans</th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ruido]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hermano, grita, normal]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[persona, gritona]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nino, grita]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[griton]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans  \\\n",
       "0                                            [ruido]                           \n",
       "1                           [hermano, grita, normal]                           \n",
       "2                                 [persona, gritona]                           \n",
       "3                                      [nino, grita]                           \n",
       "4                                           [griton]                           \n",
       "\n",
       "   explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval  \n",
       "0                                                  3                      \n",
       "1                                                  3                      \n",
       "2                                                  3                      \n",
       "3                                                  3                      \n",
       "4                                                  3                      "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtengo los valores de respuestas limpias y sus evaluaciones en un dataframe\n",
    "df_base = df[['explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans', 'explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval']]\n",
    "df_base.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe no evaluado a 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans</th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[nolo, xd]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[tocan, grito]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[pegan, pone, gritar]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[pasa, gritando, pensativo, ajajjajaj]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[grita]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans  \\\n",
       "31                                         [nolo, xd]                           \n",
       "36                                     [tocan, grito]                           \n",
       "41                              [pegan, pone, gritar]                           \n",
       "42             [pasa, gritando, pensativo, ajajjajaj]                           \n",
       "43                                            [grita]                           \n",
       "\n",
       "    explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval  \n",
       "31                                                  1                      \n",
       "36                                                  2                      \n",
       "41                                                  2                      \n",
       "42                                                  2                      \n",
       "43                                                  2                      "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecciono todas las rows que esten evaluadas con todo menos 3\n",
    "df_not_three = df_base.loc[df_base['explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval'] < 3]\n",
    "df_not_three.head(5)\n",
    "#df1 = df1.reset_index()\n",
    "#df_not_three = list(map(cleanString, df1['explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe evaluado ==  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans</th>\n",
       "      <th>explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ruido]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hermano, grita, normal]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[persona, gritona]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[nino, grita]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[griton]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans  \\\n",
       "0                                            [ruido]                           \n",
       "1                           [hermano, grita, normal]                           \n",
       "2                                 [persona, gritona]                           \n",
       "3                                      [nino, grita]                           \n",
       "4                                           [griton]                           \n",
       "\n",
       "   explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval  \n",
       "0                                                  3                      \n",
       "1                                                  3                      \n",
       "2                                                  3                      \n",
       "3                                                  3                      \n",
       "4                                                  3                      "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecciono todas las rows que esten evaluadas 3\n",
    "df_three = df_base.loc[df_base['explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval'] == 3]\n",
    "df_three.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
