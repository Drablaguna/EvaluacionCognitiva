{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud Notebook\n",
    "#### En este notebook podremos analizar con más facilidad la similitud entre las respuestas calificadas con mayor calificación con las de menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importo librerias\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hunspell import Hunspell\n",
    "\n",
    "h = Hunspell('es_MX', hunspell_data_dir = 'C:/Python/Lib/site-packages/dictionaries')\n",
    "spa_lex = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con esto evito que se impriman 'errores'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos el CSV original como un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"C:/Users/Drablaguna/Desktop/UNAM/EvaluacionCognitiva/Bases de datos/Secundaria/SECUNDARIA_TODO.csv\")\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Drablaguna/Desktop/UNAM/EvaluacionCognitiva/cuenta_de_palabras/WORDCOUNT_SECUNDARIA.csv\")\n",
    "\n",
    "#txt_file = open(\"C:/Users/Drablaguna/Desktop/ResFile.txt\", \"a+\")\n",
    "#txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones para eliminar caracteres de puntuacion\n",
    "def evalChar(x):\n",
    "    if x in [\",\",\".\",\":\",\";\",\"`\",\"'\",'\"',\"(\",\")\",\"-\",\"_\",\"~\",\"/\",\"?\",\"¿\",\"=\",\"[\",\"]\",\"\\n\",\"\\r\"]:\n",
    "        x = \"\"\n",
    "    return x\n",
    "def cleanString(x):\n",
    "    s = map(evalChar, x)\n",
    "    s = \"\".join(list(s))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para inicializar el diccionario de resultados, para tener resultados diferentes por prueba\n",
    "def start_dict():\n",
    "    dic = {\n",
    "    'token_base': [],      # tokens originales\n",
    "    'token_toCompare': [], # token con el que se comparo\n",
    "    'similarity': [],      # valor de la similitud de ambos\n",
    "    'total_sim_mean': 0    # promedio similitud por frase\n",
    "    }\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para analizar\n",
    "def analize(base, comparer):\n",
    "    dic = start_dict()\n",
    "    for token_base in base:\n",
    "        for token_toCompare in comparer:\n",
    "            simil = token_base.similarity(token_toCompare)\n",
    "            dic['token_base'].append(token_base.text)\n",
    "            dic['token_toCompare'].append(token_toCompare.text)\n",
    "            dic['similarity'].append(simil)\n",
    "    # genero el promedio de similitud que fue extraido de la sim de cada token con token\n",
    "    dic['total_sim_mean'] = np.mean(dic['similarity'])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para analizar rapido y mostrar similitud\n",
    "def quickize(master_row, slave_row):\n",
    "#     master_row = checkSentence(master_row,h)\n",
    "#     slave_row = checkSentence(slave_row,h)\n",
    "    master_row = spa_lex(master_row)\n",
    "    slave_row = spa_lex(slave_row)\n",
    "    dic = analize(master_row, slave_row)\n",
    "    return dic\n",
    "\n",
    "# funcion para imprimir las palabras que se compararon y sus similitudes\n",
    "def print_zip(dic):\n",
    "    print(list(zip(dic['token_base'], dic['token_toCompare'], dic['similarity'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para elegir la palabra correcta entre sus sugerencias\n",
    "def checkWord(word, hun_dic, original_sentence):\n",
    "    if not hun_dic.spell(word): # si la palabra NO se encuentra en el diccionario hunspell\n",
    "        n = 0\n",
    "        a = hun_dic.suggest(word)\n",
    "        print(\"=> \" + original_sentence)\n",
    "        for x in a:\n",
    "            print(str(n) + \": \" + x)\n",
    "            n+=1\n",
    "        # se puede ingresar la respuesta correcta con el id o escribiendola manualmente\n",
    "        correct_id_word = input(\"(\"+word+\") ID respuesta correcta o ingresa la palabra: \")\n",
    "        if correct_id_word.isdigit():\n",
    "            correct = a[int(correct_id_word)]\n",
    "        else:\n",
    "            correct = correct_id_word\n",
    "    else: # si la palabra se encuentra en el diccionario hunspell\n",
    "        correct = word\n",
    "    return correct.strip().lower()\n",
    "\n",
    "# funcion para checar una oracion usando la funcion de escoger la palabra correcta\n",
    "def checkSentence(s_string, hun_dic):\n",
    "    new_sentence = []\n",
    "    for token in s_string.split(\" \"): # se evaluara cada token si es una palabra existente o no\n",
    "        correct_word = checkWord(token, hun_dic, s_string)\n",
    "        if correct_word != \"\": # se agregara a la oracion nueva solo si es algo\n",
    "            new_sentence.append(correct_word)\n",
    "    #print(\"\\n\")\n",
    "    return \" \".join(new_sentence)\n",
    "        \n",
    "# s = \"porque sii mas que nada\"\n",
    "# s = checkSentence(s,h)\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado (SECU_TODO_CLEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promedio similitud entre respuestas\n",
    "#print('\\n')\n",
    "#print(list(zip(results['token_base'], results['token_toCompare'], results['similarity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado (WORDCOUNT_SECUNDARIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui se han retirado las STOPWORDS\n",
    "# de acuerdo al siguiente criterio de quickPlotter.py usando Spacy\n",
    "# if token.is_stop or token.is_punct or token.is_quote or len(token) == 1\n",
    "#print('\\n')\n",
    "#print(list(zip(results['token_base'], results['token_toCompare'], results['similarity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORDCOUNT SECUNDARIA ===========================================================\n",
    "ans_col = \"explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_clean_ans\"\n",
    "eval_col = \"explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos_eval\"\n",
    "\n",
    "# SECU TODO CLEAN ================================================================\n",
    "# ans_col = \"explica_lo_que_quiere_decir_mi_hermanito_es_una_pelota_de_gritos\"\n",
    "# eval_col = \"a5\"\n",
    "\n",
    "# obtengo los valores de respuestas limpias y sus evaluaciones en un dataframe\n",
    "df_base = df[[ans_col, eval_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes evaluados a  3, 2, 1 y 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df head testing\n",
    "\n",
    "#df_three.head(60)\n",
    "#df_three.shape\n",
    "\n",
    "#df_two.head(3)\n",
    "#df_two.shape\n",
    "\n",
    "#df_one.head(3)\n",
    "#df_one.shape\n",
    "\n",
    "#df_zero.head(3)\n",
    "#df_zero.shape\n",
    "\n",
    "print('3 => '+str(len(df_three)))\n",
    "print('2 => '+str(len(df_two)))\n",
    "print('1 => '+str(len(df_one)))\n",
    "print('0 => '+str(len(df_zero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# funcion para imprimir rapido resultados de similitudes en una columna\n",
    "\"\"\"\n",
    "def print_df_res(df_master, df_slave, ans_col):\n",
    "    counter = 1\n",
    "    total_mean = []\n",
    "    for row in df_master[ans_col]:\n",
    "        master_row = cleanString(row) # la row limpia sin [] ni comas\n",
    "        print(\"============================== \"+master_row.upper()+\" ==============================\")\n",
    "        for line in df_slave[ans_col]:\n",
    "            slave_row = cleanString(line)\n",
    "            if slave_row != \"\": # para evitar evaluaciones donde la fila este vacia\n",
    "                r = quickize(master_row, slave_row)\n",
    "                total_mean.append(r['total_sim_mean'])\n",
    "                print(str(counter)+': '+str(r['total_sim_mean']))\n",
    "                counter+=1\n",
    "    print(np.mean(total_mean))\n",
    "\"\"\" \n",
    "\n",
    "# funcion para evaluar 1 respuesta de 3 con todo\n",
    "def print_df_res(master_row, df_slave, ans_col):\n",
    "    counter = 1\n",
    "    total_mean = []\n",
    "    master_row = cleanString(master_row) # la row limpia sin [] ni comas\n",
    "    #print(\"============================== \"+master_row.upper()+\" ==============================\")\n",
    "    for line in df_slave[ans_col]:\n",
    "        slave_row = cleanString(line)\n",
    "        if slave_row != \"\": # para evitar evaluaciones donde la fila este vacia\n",
    "            r = quickize(master_row, slave_row)\n",
    "            total_mean.append(r['total_sim_mean'])\n",
    "            #print(str(counter)+': '+str(r['total_sim_mean']))\n",
    "            print(str(r['total_sim_mean']))\n",
    "            counter+=1\n",
    "    print(f\"\\n{np.mean(total_mean)}\")\n",
    "\n",
    "def quickPrint(df, ans_col, eval_col, n1, n2):\n",
    "    df_base  = df[[ans_col, eval_col]]\n",
    "    \n",
    "    df_three = df_base.loc[df_base[eval_col] == 3]\n",
    "    df_two   = df_base.loc[df_base[eval_col] == 2]\n",
    "    df_one   = df_base.loc[df_base[eval_col] == 1]\n",
    "    df_zero  = df_base.loc[df_base[eval_col] == 0]\n",
    "    \n",
    "    #three_ans = \"por que hace mucho ruido\"\n",
    "    three_ans = \"porque grita mucho\"\n",
    "    \n",
    "    # 3 con 3\n",
    "    if n1 == 3 and n2 == 3:\n",
    "        print_df_res(three_ans, df_three, ans_col)\n",
    "        \n",
    "    # 3 con 2\n",
    "    if n1 == 3 and n2 == 2:\n",
    "        print_df_res(three_ans, df_two, ans_col)\n",
    "        \n",
    "    # 3 con 1\n",
    "    if n1 == 3 and n2 == 1:\n",
    "        print_df_res(three_ans, df_one, ans_col)\n",
    "        \n",
    "    # 3 con 0\n",
    "    if n1 == 3 and n2 == 0:\n",
    "        print_df_res(three_ans, df_zero, ans_col)\n",
    "    \n",
    "    \n",
    "    # 2 con 2\n",
    "    if n1 == 2 and n2 == 2:\n",
    "        print_df_res(three_ans, df_two, ans_col)\n",
    "        \n",
    "    # 2 con 1\n",
    "    if n1 == 2 and n2 == 1:\n",
    "        print_df_res(three_ans, df_one, ans_col)\n",
    "        \n",
    "    # 2 con 0\n",
    "    if n1 == 2 and n2 == 0:\n",
    "        print_df_res(three_ans, df_zero, ans_col)\n",
    "        \n",
    "    \n",
    "    # 1 con 1\n",
    "    if n1 == 1 and n2 == 1:\n",
    "        print_df_res(three_ans, df_one, ans_col)\n",
    "        \n",
    "    # 1 con 0\n",
    "    if n1 == 1 and n2 == 0:\n",
    "        print_df_res(three_ans, df_zero, ans_col)\n",
    "        \n",
    "    \n",
    "    # 0 con 0\n",
    "    if n1 == 0 and n2 == 0:\n",
    "        print_df_res(three_ans, df_zero, ans_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quickPrint(df, ans_col, eval_col, 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
